{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f7ef20f0-722f-4240-8a79-437d4a3b8832",
      "metadata": {
        "id": "f7ef20f0-722f-4240-8a79-437d4a3b8832"
      },
      "source": [
        "## Assignment 3: $k$ Nearest Neighbor\n",
        "\n",
        "**Do two questions.**\n",
        "\n",
        "`! git clone https://github.com/DS3001/knn`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/DS3001/knn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkBO39xnLyV7",
        "outputId": "eede8da4-511f-4316-b49f-d7ce1b773b33"
      },
      "id": "rkBO39xnLyV7",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'knn' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "-bC5PIoXLyd_"
      },
      "id": "-bC5PIoXLyd_",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "71c9e0b8-17f5-4ff9-9c76-2034bffe8d5c",
      "metadata": {
        "id": "71c9e0b8-17f5-4ff9-9c76-2034bffe8d5c"
      },
      "source": [
        "**Q1.** This question is a case study for $k$ nearest neighbor The target variable `y` is `price` and the features are `year` and `mileage`.\n",
        "\n",
        "1. Load the `./data/USA_cars_datasets.csv`. Keep the following variables and drop the rest: `price`, `year`, `mileage`. Are there any `NA`'s to handle? Look at the head and dimensions of the data.\n",
        "2. Maxmin normalize `year` and `mileage`.\n",
        "3. Split the sample into ~80% for training and ~20% for evaluation.\n",
        "4. Use the $k$NN algorithm and the training data to predict `price` using `year` and `mileage` for the test set for $k=3,10,25,50,100,300$. For each value of $k$, compute the mean squared error and print a scatterplot showing the test value plotted against the predicted value. What patterns do you notice as you increase $k$?\n",
        "5. Determine the optimal $k$ for these data.\n",
        "6. Describe what happened in the plots of predicted versus actual prices as $k$ varied, taking your answer into part 6 into account. (Hint: Use the words \"underfitting\" and \"overfitting\".)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Load the ./data/USA_cars_datasets.csv. Keep the following variables and drop the rest: price, year, mileage. Are there any NA's to handle? Look at the head and dimensions of the data.**"
      ],
      "metadata": {
        "id": "EgXPw6-lLmQx"
      },
      "id": "EgXPw6-lLmQx"
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('./knn/data/USA_cars_datasets.csv', low_memory = False)"
      ],
      "metadata": {
        "id": "Bo71EPjoL4uy"
      },
      "id": "Bo71EPjoL4uy",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['price','year','mileage']\n",
        "df = df[columns]"
      ],
      "metadata": {
        "id": "SfAuQFU2MDyF"
      },
      "id": "SfAuQFU2MDyF",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head(), \"\\n\")\n",
        "print(df.shape)\n",
        "print(df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM4XpRRlMUUK",
        "outputId": "8ca2f6c6-8936-4272-ca82-75391537be04"
      },
      "id": "AM4XpRRlMUUK",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   price  year  mileage\n",
            "0   6300  2008   274117\n",
            "1   2899  2011   190552\n",
            "2   5350  2018    39590\n",
            "3  25000  2014    64146\n",
            "4  27700  2018     6654 \n",
            "\n",
            "(2499, 3)\n",
            "              price         year       mileage\n",
            "count   2499.000000  2499.000000  2.499000e+03\n",
            "mean   18767.671469  2016.714286  5.229869e+04\n",
            "std    12116.094936     3.442656  5.970552e+04\n",
            "min        0.000000  1973.000000  0.000000e+00\n",
            "25%    10200.000000  2016.000000  2.146650e+04\n",
            "50%    16900.000000  2018.000000  3.536500e+04\n",
            "75%    25555.500000  2019.000000  6.347250e+04\n",
            "max    84900.000000  2020.000000  1.017936e+06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No NA's to consider. There are 2499 rows and the count for each column is 2499, so each index has a value."
      ],
      "metadata": {
        "id": "zV3oVx0DMolC"
      },
      "id": "zV3oVx0DMolC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Maxmin normalize year and mileage.**"
      ],
      "metadata": {
        "id": "MEIhuscSLmeR"
      },
      "id": "MEIhuscSLmeR"
    },
    {
      "cell_type": "code",
      "source": [
        "def maxmin(z):\n",
        "    z = (z-min(z))/(max(z)-min(z))\n",
        "    return(z)"
      ],
      "metadata": {
        "id": "a1bzOVOxNpzO"
      },
      "id": "a1bzOVOxNpzO",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['year'] = maxmin(df['year'])\n",
        "df['mileage'] = maxmin(df['mileage'])\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJudQH9jNHAZ",
        "outputId": "c19c5f72-c6ab-4fb1-b49f-4c285b9085ae"
      },
      "id": "gJudQH9jNHAZ",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   price      year   mileage\n",
            "0   6300  0.744681  0.269287\n",
            "1   2899  0.808511  0.187194\n",
            "2   5350  0.957447  0.038892\n",
            "3  25000  0.872340  0.063016\n",
            "4  27700  0.957447  0.006537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Split the sample into ~80% for training and ~20% for evaluation.**"
      ],
      "metadata": {
        "id": "h7oI19Z9LTVj"
      },
      "id": "h7oI19Z9LTVj"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "y = df['price']\n",
        "X = df.drop('price', axis = 1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, # Feature and target variables\n",
        "                                                    test_size=.2, # Split the sample 80 train/ 20 test\n",
        "                                                    random_state=100) # For"
      ],
      "metadata": {
        "id": "vgzqOKqpQS-9"
      },
      "id": "vgzqOKqpQS-9",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Use the  ùëò NN algorithm and the training data to predict price using year and mileage for the test set for  ùëò=3,10,25,50,100,300 . For each value of  ùëò , compute the mean squared error and print a scatterplot showing the test value plotted against the predicted value. What patterns do you notice as you increase  ùëò ?**"
      ],
      "metadata": {
        "id": "u7lKB3-iLTko"
      },
      "id": "u7lKB3-iLTko"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "SSE  = np.zeros(10000)\n",
        "for k in [3,10,25,50,100,300]:\n",
        "  model = KNeighborsRegressor(n_neighbors=k)\n",
        "  fitted_model = model.fit(X,y)\n",
        "  y_hat = fitted_model.predict(X_test)\n",
        "  SSE[k] = np.sum( (y_test-y_hat)**2 )"
      ],
      "metadata": {
        "id": "S0uCrjjITXo7"
      },
      "id": "S0uCrjjITXo7",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.arange(0,k_bar),SSE,label='Test SSE') # Plot SSE by k\n",
        "plt.xlabel(\"k\")\n",
        "plt.ylabel(\"SSE\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('SSE')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "GgxcG4CyU3g2",
        "outputId": "d0ee0931-dbfd-455a-8d50-2ef9c65e8255"
      },
      "id": "GgxcG4CyU3g2",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'k_bar' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-6ad3d0824fbd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSSE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Test SSE'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Plot SSE by k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"k\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SSE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'upper right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SSE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'k_bar' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.Determine the optimal  ùëò  for these data.**"
      ],
      "metadata": {
        "id": "Nsp7Hx6VLTsy"
      },
      "id": "Nsp7Hx6VLTsy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.Describe what happened in the plots of predicted versus actual prices as  ùëò  varied, taking your answer into part 6 into account. (Hint: Use the words \"underfitting\" and \"overfitting\".)**"
      ],
      "metadata": {
        "id": "z6hIT_WxLT01"
      },
      "id": "z6hIT_WxLT01"
    },
    {
      "cell_type": "markdown",
      "id": "010b57f7-bf4f-4494-b54c-49c4f3ae3ab9",
      "metadata": {
        "id": "010b57f7-bf4f-4494-b54c-49c4f3ae3ab9"
      },
      "source": [
        "**Q2.** This question is a case study for $k$ nearest neighbor. The data for the question include:\n",
        "\n",
        "- age: age of the patient (years)\n",
        "- anaemia: decrease of red blood cells or hemoglobin (boolean)\n",
        "- high blood pressure: if the patient has hypertension (boolean)\n",
        "- creatinine phosphokinase (CPK): level of the CPK enzyme in the blood (mcg/L)\n",
        "- diabetes: if the patient has diabetes (boolean)\n",
        "- ejection fraction: percentage of blood leaving the heart at each contraction (percentage)\n",
        "- platelets: platelets in the blood (kiloplatelets/mL)\n",
        "- sex: woman or man (binary)\n",
        "- serum creatinine: level of serum creatinine in the blood (mg/dL)\n",
        "- serum sodium: level of serum sodium in the blood (mEq/L)\n",
        "- smoking: if the patient smokes or not (boolean)\n",
        "- time: follow-up period (days)\n",
        "- death event: if the patient deceased during the follow-up period (boolean)\n",
        "\n",
        "1. Load the `./data/heart_failure_clinical_records_dataset.csv`. Are there any `NA`'s to handle? use `.drop()` to remove `time` from the dataframe.\n",
        "2. Make a correlation matrix. What variables are strongly associated with a death event?\n",
        "3. For the dummy variables `anaemia`, `diabetes`, `high_blood_pressure`, `sex`, and `smoking`, compute a summary table of `DEATH_EVENT` grouped by the variable. For which variables does a higher proportion of the population die when the variable takes the value 1 rather than 0?\n",
        "4. On the basis of your answers from 2 and 3, build a matrix $X$ of the variables you think are most predictive of a death, and a variable $y$ equal to `DEATH_EVENT`.\n",
        "5. Maxmin normalize all of the variables in `X`.\n",
        "6. Split the sample into ~80% for training and ~20% for evaluation. (Try to use the same train/test split for the whole question, so that you're comparing apples to apples in the questions below.).\n",
        "7. Determine the optimal number of neighbors for a $k$NN regression for the variables you selected.\n",
        "8. OK, do steps 5 through 7 again, but use all of the variables (except `time`). Which model has a lower Sum of Squared Error? Which would you prefer to use in practice, if you had to predict `DEATH_EVENT`s? If you play with the selection of variables, how much does the SSE change for your fitted model on the test data? Are more variables always better? Explain your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d457e190-d273-455f-b94f-62916cb5af1c",
      "metadata": {
        "id": "d457e190-d273-455f-b94f-62916cb5af1c"
      },
      "source": [
        "**Q3.** Let's do some very basic computer vision. We're going to import the MNIST handwritten digits data and $k$NN to predict values (i.e. \"see/read\").\n",
        "\n",
        "1. To load the data, run the following code in a chunk:\n",
        "```\n",
        "from keras.datasets import mnist\n",
        "df = mnist.load_data('minst.db')\n",
        "train,test = df\n",
        "X_train, y_train = train\n",
        "X_test, y_test = test\n",
        "```\n",
        "The `y_test` and `y_train` vectors, for each index `i`, tell you want number is written in the corresponding index in `X_train[i]` and `X_test[i]`. The value of `X_train[i]` and `X_test[i]`, however, is a 28$\\times$28 array whose entries contain values between 0 and 256. Each element of the matrix is essentially a \"pixel\" and the matrix encodes a representation of a number. To visualize this, run the following code to see the first ten numbers:\n",
        "```\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "np.set_printoptions(edgeitems=30, linewidth=100000)\n",
        "for i in range(5):\n",
        "    print(y_test[i],'\\n') # Print the label\n",
        "    print(X_test[i],'\\n') # Print the matrix of values\n",
        "    plt.contourf(np.rot90(X_test[i].transpose())) # Make a contour plot of the matrix values\n",
        "    plt.show()\n",
        "```\n",
        "OK, those are the data: Labels attached to handwritten digits encoded as a matrix.\n",
        "\n",
        "2. What is the shape of `X_train` and `X_test`? What is the shape of `X_train[i]` and `X_test[i]` for each index `i`? What is the shape of `y_train` and `y_test`?\n",
        "3. Use Numpy's `.reshape()` method to covert the training and testing data from a matrix into an vector of features. So, `X_test[index].reshape((1,784))` will convert the $index$-th element of `X_test` into a $28\\times 28=784$-length row vector of values, rather than a matrix. Turn `X_train` into an $N \\times 784$ matrix $X$ that is suitable for scikit-learn's kNN classifier where $N$ is the number of observations and $784=28*28$ (you could use, for example, a `for` loop).\n",
        "4. Use the reshaped `X_test` and `y_test` data to create a $k$-nearest neighbor classifier of digit. What is the optimal number of neighbors $k$? If you can't determine this, play around with different values of $k$ for your classifier.\n",
        "5. For the optimal number of neighbors, how well does your predictor perform on the test set?\n",
        "6. So, this is how computers \"see.\" They convert an image into a matrix of values, that matrix becomes a vector in a dataset, and then we deploy ML tools on it as if it was any other kind of tabular data. To make sure you follow this, invent a way to represent a color photo in matrix form, and then describe how you could convert it into tabular data. (Hint: RGB color codes provide a method of encoding a numeric value that represents a color.)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}